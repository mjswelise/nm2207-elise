---
title: "How Does Genre Classification Work? A Study of Spotify Data"
format: html
editor: visual
---

> **"Music expresses that which cannot be put into words and that which cannot remain silent**." - Victor Hugo

## Introduction

In delving into the mechanics of genre classification within the realm of music, this study takes a comprehensive look at the inner workings of Spotify's genre classification system. Extending its scope across six decades, from 1957 to 2020, this musical exploration serves as a time-traveling odyssey, unraveling the intricate threads that define how music genres not only mold themselves but are intricately woven into the fabric of shifting industry dynamics and evolving listener preferences.

The significance of this exploration is heightened amidst the rapidly transforming landscape of the music industry. As indicated by the Recording Industry Association of America (2022), the surge in revenues from streaming music has been remarkable, constituting an astonishing 84% of total music revenues in the initial half of 2021. At the forefront of this revolution stands Spotify, acknowledged as the "world's largest music streaming provider in terms of global paid-for subscriptions" (Music Business Worldwide, 2023), with projections foreseeing sustained dominance throughout the 2020s (Goldman Sachs, 2023). Simultaneously, the advent of digital music platforms has led to a departure from rigid "hard genre categorisation" to a more adaptable "mood-based categorisation" approach. This strategic shift aims at fostering new music through effective exposure, as highlighted by industry observers (The Guardian, 2017).

Thus, scrutinising the metamorphosis of music genre categorization on Spotify in response to released songs transcends mere academic inquiry. It serves as a focal point through which we can glean insights into the dynamic interplay of change and continuity within the music industry. This understanding not only informs business strategies but also provides valuable perspectives on the perpetually evolving musical landscape, particularly within the context of our increasingly digitised society.

## 'Tidying' Data

```{r setup, warning = FALSE, error = FALSE, message = FALSE, echo = FALSE}
library(tidyverse)
library(ggplot2)
library(tibble)
library(dplyr)
library(cowplot)
library(lubridate)
library(corrplot)
library(gridExtra)
library(rpart)
library(rpart.plot)
library(kableExtra)

# view of data set
spotify_songs <- read.csv("spotify_songs.csv")
```

First, to isolate the tracks, genres, and audio features for easy processing, I created a new data-set with the relevant variables. I also created a list of genres and the respective quantity of songs for each.

```{r, echo = FALSE, eval = TRUE}
# view column names
column_names <- data.frame(Variable = names(spotify_songs))

# new dataset for variables
spotify_genres <- spotify_songs %>%
                  select(track_name, track_album_release_date, playlist_genre, playlist_subgenre)

# new dataset for features
audio_features <- spotify_songs %>%
                  select(track_name, track_album_release_date, danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, duration_ms)

# what are the individual genres
summary_genres <- spotify_genres %>%
                  count(playlist_genre)
```

```{r, echo = FALSE, eval = TRUE}
count <- summary_genres %>%
  knitr::kable()
count
```

Using ggplot, to get a sensing of the dataset, I then plot the data into graphs to find the overall trends in genre popularity, subgenre dynamics, and the counts of each subgenre within each genre.

Find it in my interactive Shiny app below!

```{=html}
<iframe height="800" width="100%" frameborder="no" src="https://mjswelise.shinyapps.io/2207dataset_overview/"> </iframe>
```
## Defining Genre

In my quest to determine what makes a 'genre', I first condensed the data-sets 'by year' from YYYY-MM-DD. Here, I found that there was an insufficient population size of songs from before 1966 (i.e. \<30 instances), and chose to filter the data that way to ensure a more accurate output from my dataset.

```{r, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
spotify_genres_byyear <- spotify_genres %>% 
  mutate(track_album_release_date = ymd(track_album_release_date)) %>%
  mutate(year = year(track_album_release_date)) %>%
  select(track_name, track_album_release_date, playlist_genre, playlist_subgenre, year) %>%
  arrange(year)

most_byyear <- head(spotify_genres_byyear, n = 30947)
byyear1 <- most_byyear %>% 
           select(track_name, playlist_genre, playlist_subgenre, year)

year_only <- tail(spotify_genres_byyear, n = 1885)
byyear2 <- year_only %>%
           select(track_name, playlist_genre, playlist_subgenre, track_album_release_date) %>%
           rename(year = track_album_release_date)

final_byyear <- rbind(byyear1, byyear2) %>%
                arrange(year)

# split the datasets by year
yearly_datasets <- final_byyear %>%
  group_split(year)

dataset_names <- unique(final_byyear$year)

for (i in seq_along(yearly_datasets)) {
  assign(paste0("spotify_dataset_", dataset_names[i]), yearly_datasets[[i]])
}

# hence, may need to eliminate datasets before 1966 - minimum sample size as a population size of 30.
cleaned_datasets <- tail(final_byyear, n = 32793)
```

#### Density of Audio Features by Genre

Then, I created a histogram to determine the density of audio features by genre. Here, we can observe that, collectively, the songs in the dataset lean towards characteristics such as low acousticness, liveness, instrumentalness, and speechiness, coupled with higher danceability, energy, and loudness.

On a genre-specific level, EDM tends to feature lower acoustic elements, heightened energy, and subdued valence, conveying a sense of melancholy. In contrast, Latin music is characterised by elevated valence, indicating a positive and cheerful mood, along with pronounced danceability. Rap songs stand out for their high speechiness and danceability, while rock compositions favor live recordings and exhibit lower danceability.

Examining song durations by genre reveals that Pop, Latin, and EDM songs tend to be shorter, while R&B, rap, and rock songs are associated with longer durations.

Analysis of the histogram suggests that elements like energy, valence, tempo, and danceability hold significant potential for distinguishing between genres, whereas instrumentalness and key may contribute less prominently to genre classification.

```{r, echo = TRUE, eval = TRUE, fig.height = 6, fig.width = 8, warning = FALSE}
# density of audio features by genre, probably change this to shiny
audio_features <- names(spotify_songs)[c(12:23)]

spotify_songs %>%
  select(c('playlist_genre', audio_features)) %>%
  pivot_longer(cols = audio_features) %>%
  ggplot(aes(x = value)) +
  geom_histogram(aes(fill = playlist_genre), color = "black", alpha = 0.5, bins = 10) +
  facet_wrap(~name, ncol = 4, scales = 'free') +
  labs(title = 'Histogram - Audio Features by Genre', y = 'Density') +
  theme(axis.text.y = element_blank())
```

#### Correlation between Individual Audio Features

In addition, I found the correlation between individual audio features, as well as of individual genres (in the next section!). I wanted to evaluate any further interdependencies among these musical characteristics, as well as to understand the correlation patterns specific to distinct genres, to shed light on how the musical attributes within each genre are interconnected.

```{r, echo = TRUE, eval = TRUE, fig.height = 8, fig.width = 8}
# correlation between audio features
spotify_songs %>%
  select(audio_features) %>%
  scale() %>%
  cor() %>%
  corrplot::corrplot(method = 'color', 
                     order = 'hclust', 
                     type = 'lower', 
                     diag = TRUE,
                     col = COL2('PuOr', 100),
                     tl.col = 'black',
                     addCoef.col = "black",
                     number.cex = 0.5,
                     title = 'Correlation between Audio Features',
                     cex.main = 0.8,
                     mar = c(3,3,3,3),
                     tl.cex = 0.5,
                     cl.ratio = 0.2,
                     cl.cex = 0.5)
```

Across the audio features, energy and loudness seem especially correlated, with a score of +0.68. As such, perhaps, here, we can establish that loudness is less relevant in a genre classification sense, given that energy is still strongly associated with having distinctions with other audio features (i.e. negatively correlated with acousticness, at -0.54, which can be used as a differentiating factor).

Other noticeable correlations can include danceability and valence, which are positively correlated at +0.33.

#### Correlation between Individual Genres

```{r, echo = TRUE, eval = TRUE, fig.height = 8, fig.width = 8}
# correlation between genres
genre_median <- spotify_songs %>%
  group_by(playlist_genre) %>%
  summarise_if(is.numeric, median, na.rm = TRUE)

genre_overview <- genre_median %>%
  select(audio_features, -mode) %>%
  scale() %>%
  t() %>%
  as.matrix() %>%
  cor()

colnames(genre_overview) <- genre_median$playlist_genre
row.names(genre_overview) <- genre_median$playlist_genre

genre_overview %>% corrplot::corrplot(method = 'color', 
                     order = 'hclust',
                     type = 'lower',
                     tl.col = 'black',
                     diag = TRUE,
                     col = COL2('PuOr', 100),
                     addCoef.col = "black",
                     number.cex = 0.5,
                     title = 'Correlation between Genre Features',
                     cex.main = 0.8,
                     mar = c(3,3,3,3),
                     tl.cex = 0.5,
                     cl.ratio = 0.2,
                     cl.cex = 0.5)
```

Statistically, by genre, R&B and EDM are most different from each other - they are strongly and negatively correlated by -0.77. In terms of similarities, Latin and R&B seem most similar, positively correlated by 0.45.

### So, Let's Try to Classify Genres?

In light of this information, the analysis has provided valuable insights into the characteristics that define each genre. Now, the focus shifts towards creating a quantifiable method for genre categorisation.

To achieve this, I planned to identify the median value of each audio feature for each genre and compare it with the overall median of the entire dataset. The exploration of median values for each genre's audio features in comparison to the overall dataset median will serve as a quantitative basis for discerning the distinctive attributes that define each genre.

```{r, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE}
# correlation between genre and audio features
ss1 <- spotify_songs %>%
  mutate(track_album_release_date = ymd(track_album_release_date)) %>%
  mutate(year = year(track_album_release_date)) %>%
  select(track_name, track_popularity, track_album_release_date, year, 
         playlist_genre, danceability, energy, key, speechiness, acousticness, 
         instrumentalness, liveness, valence, tempo) %>%
  arrange(desc(year))

# clean data without year attached
ss_fin <- head(ss1, n = 30947)

# find overall median value of various audio features
ss_median <- summarise_all(ss_fin, median, na.rm = TRUE)
medians <- ss_median %>%
  select(track_popularity, danceability, energy, key, speechiness, acousticness, instrumentalness, liveness, valence, tempo)

# write a function to compare the values and print them in a table format
calculate_summary_comparison <- function(x, genre_filter) {
  genre_ss <- ss_fin %>%
    filter(playlist_genre == genre_filter) %>%
    select(track_popularity, danceability, energy, key, speechiness, acousticness, instrumentalness, liveness, valence, tempo) 

  genre_summary <- summarise_all(genre_ss, median, na.rm = TRUE)
  
  medians_comparison <- t(rbind(medians, genre_summary) %>%
    mutate(genre = rep(c("medians", genre_filter), each = nrow(medians))))

  comparison_result <- cbind(medians_comparison, subtraction_result = 
                    apply(medians_comparison[-nrow(medians_comparison), ], 1, function(row) 
                    as.numeric(row[1:(length(row)-1)]) - 
                    as.numeric(row[2:length(row)])))
  comparison_result <- as.data.frame(comparison_result)

  header <- as.character(comparison_result[nrow(comparison_result), ])
  comparison_result <- comparison_result[-nrow(comparison_result), ]
  colnames(comparison_result) <- header
  colnames(comparison_result)[3] <- "subtracted_value"

  comparison_result <- comparison_result %>%
    knitr::kable()
  
  return(comparison_result)
}

# to find,
pop_comparison <- calculate_summary_comparison(x = "pop", genre_filter = "pop")
pop_comparison # popularity, energy

rnb_comparison <- calculate_summary_comparison(x = "r&b", genre_filter = "r&b")
rnb_comparison # danceability, speechiness, acousticness, valence

rap_comparison <- calculate_summary_comparison(x = "rap", genre_filter = "rap")
rap_comparison # popularity, danceability, speechiness, acousticness, valence

edm_comparison <- calculate_summary_comparison(x = "edm", genre_filter = "edm")
edm_comparison # energy, instrumentalness, liveness, tempo

rock_comparison <- calculate_summary_comparison(x = "rock", genre_filter = "rock")
rock_comparison # energy, instrumentalness, liveness, valence, tempo

latin_comparison <- calculate_summary_comparison(x = "latin", genre_filter = "latin")
latin_comparison # popularity, danceability, energy, speechiness, acousticness, valence
```

As such, the preliminary defining variables for each genre are:

'Pop' - popularity, energy

'R&B' - danceability, speechiness, acousticness, valence

'Rap' - popularity, danceability, speechiness, acousticness, valence

'EDM' - energy, instrumentalness, liveness, tempo

'Rock' - energy, instrumentalness, liveness, valence, tempo

'Latin' - energy, instrumentalness, liveness, valence, tempo

## Test the Predictive Algorithm

In the pursuit of evaluating the effectiveness of my predictive algorithm (i.e. of using audio features to predict genre), the decision was made to employ decision trees as the analytical framework. Decision trees, renowned for their interpretability and efficiency in handling classification tasks, were selected to discern the feasibility of classifying music genres based on the prominence of audio features.

The utilisation of decision trees aligns with the objective of unveiling discernible patterns within the intricate interplay of audio features, providing valuable insights into the distinctive characteristics that define various musical genres. This section undertakes the crucial task of testing the predictive capabilities of the algorithm, shedding light on its potential to accurately classify genres based on the nuanced nuances of audio feature prominence.

Here, the decision tree was structured as such:

1.  **Energy Check:** If the energy level of the song is greater than 0.721, the function classifies it as "pop." This suggests that songs with high energy levels are predominantly associated with the pop genre.

2.  **Popularity and Danceability Check:** If the song's track popularity exceeds 45, a secondary check is initiated. If the danceability of the song surpasses 0.674, it is categorized as "latin." Otherwise, it is classified as "pop." This implies that highly popular songs with elevated danceability are likely to fall into the Latin genre.

3.  **Danceability Check:** If the danceability of the song is greater than 0.674 (but the popularity condition is not met), it is labeled as "r&b." This indicates that songs with high danceability but not necessarily high popularity are inclined towards the R&B genre.

4.  **Valence Check:** If the valence (positivity) of the song exceeds 0.506 (but neither the energy nor danceability conditions are met), the function assigns the genre as "rock." This suggests that songs with a positive emotional tone are associated with the rock genre.

5.  **Default Case:** If none of the above conditions are met, the function defaults to classifying the song as "edm." This serves as a catch-all category for songs that do not meet the specific criteria outlined for the other genres.

Notably, I had initially set the initial default case to classify songs as "pop." However, recognising the overrepresentation of pop songs within the dataset, the default classification was adjusted to "edm" instead. This modification aims to ensure a more balanced and nuanced approach to genre assignment, considering the diversity of the dataset and avoiding a disproportionate bias toward the pop genre.

```{r, echo = TRUE, eval = TRUE, fig.height = 20, fig.width = 7}
classify_genre <- function(song) {
  if (song["energy"] > 0.721) {
    return("pop")
  } else if (song["track_popularity"] > 45) {
    if (song["danceability"] > 0.674) {
      return("latin")
    } else {
      return("pop")
    }
  } else if (song["danceability"] > 0.674) {
    return("r&b")
  } else if (song["valence"] > 0.506) {
    return("rock")
  } else {
    return("edm")
  }
}

spotify_songs$predicted_genre <- apply(spotify_songs, 1, classify_genre)
predictions <- spotify_songs %>%
  select(track_name, track_popularity, track_album_release_date, 
         playlist_genre, predicted_genre, danceability, energy, key, 
         speechiness, acousticness, instrumentalness, liveness, valence, tempo) %>%
  mutate(match = if_else(playlist_genre == predicted_genre, TRUE, FALSE))

accuracy_predictions <- predictions %>% 
    mutate(match = ifelse(playlist_genre == predicted_genre, TRUE, FALSE)) %>% 
    count(match) %>% 
    mutate(accuracy = n/sum(n) * 100)
accuracy_predictions
```

However, as per the accuracy table above, the percentage accuracy of classification remained to be less than 20%.

To test it again, I tried the predictive algorithm on a smaller test set, instead of the whole data-set.

```{r, echo = TRUE, eval = TRUE, fig.height = 5, fig.width = 7}
# trying with a test case of 10 random songs from the spotify_songs dataset
set.seed(123)

random_10 <- spotify_songs %>%
  sample_n(10)

random_10$predicted_genre <- apply(random_10, 1, classify_genre)
predictions10 <- random_10 %>%
  select(track_name, track_popularity, track_album_release_date, 
         playlist_genre, predicted_genre, danceability, energy, key, 
         speechiness, acousticness, instrumentalness, liveness, valence, tempo) %>%
  mutate(match = if_else(playlist_genre == predicted_genre, TRUE, FALSE)) %>%
  arrange(predicted_genre)

accuracy_predictions10 <- predictions10 %>% 
    mutate(match = ifelse(playlist_genre == predicted_genre, TRUE, FALSE)) %>% 
    count(match) %>% 
    mutate(accuracy = n/sum(n) * 100)
accuracy_predictions10
```

Nevertheless, the rate of accuracy remained quite low at 20%.

## Discussion and Conclusion

In delving into the exploration of genre classification through audio features, the complexity of the task becomes apparent. The decision tree, chosen for its transparency, is a valuable tool, yet its application to genre classification reveals intricate challenges. The research question at the core of this endeavor --- How Does Genre Classification Work? --- is met with a nuanced response, emphasising the multifaceted nature of music genres and the inherent subjectivity in their definition.

Genre classification encounters complexities stemming from the diverse and evolving nature of musical styles. The decision tree, though structured with careful considerations of energy, popularity, danceability, and valence, grapples with several less quantifiable dynamic and subjective aspects inherent to genre delineation. The challenge lies not only in deciphering the intricate relationships between audio features but also in navigating the fluid boundaries between genres.

Furthermore, the inherent subjectivity in defining genres adds a layer of complexity. Genres are not rigidly defined categories but rather fluid and context-dependent concepts. Individual perceptions, cultural influences, and temporal shifts contribute to the malleability of genre boundaries. Consequently, a single decision tree, while illuminating certain distinctions, may struggle to encapsulate the vast spectrum of musical expression.

The potential interactions among audio features, not fully captured by the decision tree, pose another layer of challenge. Music is a rich tapestry where features intertwine in intricate ways, and their combined effect contributes to the holistic experience of a genre. The decision tree, by nature, compartmentalises these features, potentially overlooking synergies that play a crucial role in genre identification.

Addressing the research question of whether genre classification can be achieved through audio feature classification, the presented decision tree offers valuable insights but falls short of providing a definitive answer. The modest accuracy rates underscore the complexity of the task and hint at the limitations of relying solely on audio features for genre classification.

In conclusion, genre classification proves to be a nuanced and challenging endeavor, influenced by the multifaceted nature of musical expression, the subjectivity inherent in genre definitions, and the intricate interactions among audio features. The decision tree, while illuminating certain aspects, prompts further exploration into more sophisticated methodologies, expanded feature sets, and a deeper integration of domain knowledge.

As the quest for accurate genre classification continues, the intersection of technology, music theory, and human perception remains a fascinating and evolving terrain.

## References

Goldman Sachs. (2023). *Music In The Air.* https://www.goldmansachs.com/intelligence/page/music-streaming-services-are-on-the-cusp-of-major-structural-change.html

Music Business Worldwide. (2023). *WHY GOLDMAN SACHS BELIEVES THAT SPOTIFY WILL REMAIN THE WORLD'S DOMINANT MUSIC STREAMING SERVICE IN 2030.* https://www.musicbusinessworldwide.com/why-goldman-sachs-believes-that-spotify-will-remain-the-worlds-dominant-music-streaming-service-in-2030/

Recording Industry Association of America. (2022). *Mid-Year 2021 RIAA Revenue Statistics.* https://www.riaa.com/wp-content/uploads/2021/09/Mid-Year-2021-RIAA-Music-Revenue-Report.pdf

The Guardian. (2017). *'They could destroy the album': how Spotify's playlists have changed music for ever.* https://www.theguardian.com/music/2017/aug/17/they-could-destroy-the-album-how-spotify-playlists-have-changed-music-for-ever
